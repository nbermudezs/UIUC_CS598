{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mashup embedding data with 22 classes\n",
    "datf1_mashup = np.load(\"processed/mashup.human-features.npy\")\n",
    "label1_mashup = np.load(\"processed/mashup.human-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mashup embedding data With 122 classes\n",
    "datf2 = np.load(\"processed-211-labels/processed/mashup.human-features.npy\")\n",
    "label2 = np.load(\"processed-211-labels/processed/mashup.human-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, f1_score,recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import datetime\n",
    "\n",
    "#Takes a scikitlearn classifier (Param: method ) as input along with with features(param: h_features) and lables (Param:h_labels)\n",
    "# runs The classifier N times (param: val_iter)\n",
    "\n",
    "def classify(method,h_features, h_labels, val_iter=3):\n",
    "    \n",
    "    print(h_features.shape, h_labels.shape)\n",
    "    print \"Training -> {0}  - Classifier running {1} times.................\".format(str(method), val_iter)   \n",
    "    \n",
    "    f1_macro=[]\n",
    "    f1_micro=[]\n",
    "    recall_macro=[]\n",
    "    recall_micro=[]\n",
    "    accuracy=[]\n",
    "    classwise_stats=[]\n",
    "    precision_macro= []\n",
    "    precision_micro=[]\n",
    "    \n",
    "    for i in range(val_iter):\n",
    "        print \"run - - - - - - - - -  {0} at: {1} \".format(i+1, datetime.datetime.now())\n",
    "    \n",
    "        x_Train, x_Test, y_Train,  y_Test = train_test_split(h_features,h_labels,test_size=0.20 ) #Train test split\n",
    "        classifier = MultiOutputClassifier(method)\n",
    "        classifier.fit(x_Train,y_Train)\n",
    "        predicted = classifier.predict(x_Test)\n",
    "        f1_macro.append(f1_score(y_Test, predicted, average='macro'))\n",
    "        f1_micro.append(f1_score(y_Test, predicted, average='micro'))\n",
    "        recall_macro.append(recall_score(y_Test, predicted, average='macro'))\n",
    "        recall_micro.append(recall_score(y_Test, predicted, average='micro'))\n",
    "        accuracy.append(accuracy_score(y_Test, predicted))\n",
    "        precision_macro.append(precision_score(y_Test, predicted, average='macro'))\n",
    "        precision_micro.append(precision_score(y_Test, predicted, average='micro'))\n",
    "                         \n",
    "        classwise_stats.append(class_metrics(predicted,y_Test))\n",
    "    \n",
    "    print \"F1 macro\"\n",
    "    print f1_macro\n",
    "    print \"F1 Micro\"\n",
    "    print f1_micro\n",
    "    \n",
    "    print \"Recall macro\"\n",
    "    print recall_macro\n",
    "    print \"Recall Micro\"\n",
    "    print recall_micro\n",
    "    \n",
    "    print \"Precision Macro\"\n",
    "    print precision_macro\n",
    "    \n",
    "    print \"Precision Micro\"\n",
    "    print precision_micro\n",
    "           \n",
    "    print \"Accuracy\"\n",
    "    print accuracy\n",
    "    \n",
    "    print \"Classwise Metrics\"\n",
    "    print classwise_stats\n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    return accuracy, predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates class wise metrics. Input - labels predicted by a classifier (param: predicted) \n",
    "# and the true labels ( pram: true_labels)\n",
    "def class_metrics(predicted, true_labels):\n",
    "    if predicted.shape != true_labels.shape:\n",
    "        print \"Incompatible shapes. Unable to compute classwise metrics\"\n",
    "    #print \"Total Classes: {0}\".format(predicted.shape[1])\n",
    "    f1_macro=[]\n",
    "    recall_macro=[]\n",
    "    accuracy=[]\n",
    "    for k in range(predicted.shape[1]):\n",
    "        \n",
    "        recall_macro.append(recall_score(predicted[:,k], true_labels[:,k], average='macro'))\n",
    "        \n",
    "        f1_macro.append(f1_score(predicted[:,k], true_labels[:,k], average='macro'))\n",
    "        \n",
    "        accuracy.append(accuracy_score(predicted[:,k], true_labels[:,k]))\n",
    "#     print \" \"    \n",
    "#     print \" *Classwise Recall macro\"\n",
    "#     print recall_macro\n",
    "    \n",
    "#     print \"............................. \"\n",
    "#     print \" *Classwise F1 Macro\"\n",
    "#     print f1_macro\n",
    "    \n",
    "#     print \"...............................\"\n",
    "#     print \" *Classwise Accuracy\"\n",
    "#     print accuracy\n",
    "    classwise_stats= {\"Recall_macro\":recall_macro, \"F1_Macro\":f1_macro,\"Classwise_Accuracy\": accuracy  }\n",
    "    return classwise_stats\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "scoresvc, predictedsvc = classify(SVC(C=1.0, kernel='rbf'), datf1_mashup, label1_mashup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "score, predicted = classify(LogisticRegression(), datf1_mashup, label1_mashup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((9190L, 800L), (9190L, 22L))\n",
      "Training -> GaussianNB(priors=None, var_smoothing=1e-09)  - Classifier running 3 times.................\n",
      "run - - - - - - - - -  1 at: 2018-11-17 15:20:30.073000 \n",
      "run - - - - - - - - -  2 at: 2018-11-17 15:20:36.389000 \n",
      "run - - - - - - - - -  3 at: 2018-11-17 15:20:42.612000 \n",
      "F1 macro\n",
      "[0.13867247447074355, 0.14409304545789883, 0.135088983862502]\n",
      "F1 Micro\n",
      "[0.33913745387453875, 0.3841113366193099, 0.34543773933711874]\n",
      "Recall macro\n",
      "[0.3303945550559915, 0.3140317043526384, 0.28971660114751363]\n",
      "Recall Micro\n",
      "[0.6123256298146992, 0.6990291262135923, 0.5411667356226727]\n",
      "Precision Macro\n",
      "[0.12115800277712534, 0.12123049627074925, 0.12456501291394426]\n",
      "Precision Micro\n",
      "[0.2345108045610398, 0.2648117054449508, 0.25368502715283164]\n",
      "Accuracy\n",
      "[0.05984766050054407, 0.06637649619151251, 0.06964091403699674]\n",
      "Classwise Metrics\n",
      "[{'Recall_macro': [0.49994845929459764, 0.5137403544171552, 0.4976155444002077, 0.500727867450503, 0.5008676439597493, 0.49724033717187666, 0.5014467350577785, 0.5021701794014972, 0.4999235286920347, 0.5040935357469376, 0.5040515598655134, 0.5039831926382234, 0.5119485947131961, 0.5074881504826663, 0.5056062132867992, 0.4966492105485691, 0.5026480483562854, 0.49864294242697804, 0.5027997010463378, 0.49791218375476914, 0.49461711584040247, 0.49753737962095146], 'F1_Macro': [0.44458472559740786, 0.43267858506177226, 0.4442004074357016, 0.45954603898191165, 0.48291985720198677, 0.41456892913948123, 0.39458090341744373, 0.4844319775596073, 0.462717731233586, 0.45321925906958177, 0.4726563507414571, 0.41122594511161104, 0.45596104642783536, 0.4646690830669335, 0.4677906028791855, 0.4471987419796698, 0.40985056542810994, 0.46487801542984397, 0.447163793078564, 0.48587281252434783, 0.43560795979320854, 0.495054945054945], 'Classwise_Accuracy': [0.7921653971708379, 0.6414581066376496, 0.6898803046789989, 0.6496191512513602, 0.7159956474428727, 0.6583242655059848, 0.5723612622415669, 0.8264417845484222, 0.794341675734494, 0.7421109902067464, 0.8063112078346029, 0.4815016322089227, 0.6882480957562568, 0.7295973884657236, 0.7464635473340587, 0.7094668117519043, 0.4809575625680087, 0.79379760609358, 0.7165397170837867, 0.8808487486398259, 0.721436343852013, 0.926006528835691]}, {'Recall_macro': [0.49925925925925924, 0.5012857604000305, 0.5041559699989933, 0.5128377923040637, 0.4993483412322275, 0.49855829804812557, 0.49736655287574155, 0.4975185205310728, 0.5040224947277981, 0.5087758820937963, 0.5061893674815677, 0.5210401192937751, 0.4990603668733773, 0.497424983458503, 0.49994200192383864, 0.5042970733941136, 0.5196467546023913, 0.502872599817129, 0.5000523012552301, 0.5090947198624649, 0.5003339550357899, 0.4944166127989657], 'F1_Macro': [0.4231010671688638, 0.38213176922343717, 0.466534226648196, 0.5010442490682274, 0.49883447390932417, 0.4225218461558024, 0.39951964634277376, 0.49006730743650256, 0.4939224638551804, 0.5082295469649499, 0.5033845278725825, 0.43021327639157103, 0.42711777015461294, 0.4341863536608375, 0.4439590645220313, 0.47007476338185, 0.4274051475372805, 0.4088170038466083, 0.44358604254659584, 0.5081866033847987, 0.4724909752083666, 0.4926094537001162], 'Classwise_Accuracy': [0.7334058759521219, 0.5701849836779108, 0.7268770402611534, 0.7285092491838956, 0.8335146898803046, 0.6773667029379761, 0.5914036996735582, 0.8808487486398259, 0.8803046789989118, 0.9156692056583242, 0.9173014145810664, 0.5043525571273123, 0.6349292709466812, 0.6626768226332971, 0.6833514689880305, 0.7475516866158868, 0.5032644178454843, 0.6267682263329706, 0.7279651795429815, 0.9254624591947769, 0.8117519042437432, 0.9379760609357998]}, {'Recall_macro': [0.5016881191454949, 0.5059017388401663, 0.5086256791873376, 0.49800763652631685, 0.4970714486299996, 0.4954816921436189, 0.49570347617620614, 0.4959568652636925, 0.5094226014322867, 0.5004080430758876, 0.49577274023381807, 0.5178620309639347, 0.50099689749122, 0.5054071568506261, 0.5086445404948944, 0.512260774830662, 0.5177973814692972, 0.4971690055231248, 0.5042510342068294, 0.5106550694590669, 0.5024270362681647, 0.4968456318185723], 'F1_Macro': [0.44051069482110244, 0.39331441906701897, 0.46147915124599065, 0.4506217120994739, 0.4497375453874831, 0.4528849584424961, 0.4058302028946497, 0.48386845925209093, 0.4946127054606188, 0.49226845516860357, 0.48100207825065516, 0.43049034736358266, 0.4979552974964101, 0.5049532268236283, 0.5086542794574477, 0.478088456068268, 0.42907036677437527, 0.4158499897688634, 0.44789057135964666, 0.5113126456876457, 0.49846973015627183, 0.48332474024217864], 'Classwise_Accuracy': [0.7725788900979326, 0.5805223068552775, 0.7105549510337323, 0.6191512513601741, 0.6142546245919478, 0.7698585418933623, 0.6001088139281828, 0.8585418933623504, 0.85310119695321, 0.9064200217627857, 0.8911860718171926, 0.5070729053318824, 0.8476605005440696, 0.8661588683351469, 0.8667029379760609, 0.749727965179543, 0.5065288356909684, 0.6425462459194777, 0.7181719260065288, 0.9205658324265505, 0.9194776931447225, 0.8852013057671382]}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "score, predicted = classify(GaussianNB(),datf1_mashup, label1_mashup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# score, predicted = classify(GradientBoostingClassifier())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Graph Sage embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Graph sage embedding data with 22 classes\n",
    "dat1_graph_sage = np.load(\"processed-graphsage-mean-22-labels/graphsage-mean.human-features.npy\")\n",
    "label1_graph_sage = np.load(\"processed-graphsage-mean-22-labels/graphsage-mean.human-labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "score, predicted = classify(GaussianNB(), dat1_graph_sage,label1_graph_sage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "scoresvc, predictedsvc = classify(SVC(C=1.0, kernel='rbf'), dat1_graph_sage,label1_graph_sage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "score, predicted = classify(LogisticRegression(), dat1_graph_sage,label1_graph_sage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_py2",
   "language": "python",
   "name": "ml_py2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
